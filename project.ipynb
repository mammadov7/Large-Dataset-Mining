{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_test_small.csv')\n",
    "df['DEP_BLOCK'] = pd.Categorical(df.DEP_BLOCK).codes\n",
    "df['CARRIER_NAME'] = pd.Categorical(df.CARRIER_NAME).codes\n",
    "df['DEPARTING_AIRPORT'] = pd.Categorical(df.DEPARTING_AIRPORT).codes\n",
    "df['PREVIOUS_AIRPORT'] = pd.Categorical(df.DEP_BLOCK).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    414368\n",
       "1     87682\n",
       "Name: DEP_DEL15, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df.drop(df.index[idx[:len(idx)//5*4]])\n",
    "df.DEP_DEL15.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix : for viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "# mask = np.zeros_like(corr, dtype=np.bool)\n",
    "# mask[np.triu_indices_from(mask)] = True\n",
    "# f, ax = plt.subplots(figsize=(25, 20))\n",
    "# cmap = sns.diverging_palette(300, 20, as_cmap=True)\n",
    "\n",
    "\n",
    "# sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "#             square=False, linewidths=0.1, cbar_kws={\"shrink\": .1}, ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.to_parquet('df.parquet.gzip',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(data):\n",
    "    fig, axs = plt.subplots(5,5,figsize=(30, 30) )\n",
    "    columns = list(df.columns[:2])+list(df.columns[3:])\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "                axs[i,j].hist(data[columns[i*5+j]])\n",
    "                axs[i,j].set_title(columns[i*5+j])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['DAY_OF_WEEK', 'DISTANCE_GROUP', 'DEP_BLOCK','SEGMENT_NUMBER', 'CONCURRENT_FLIGHTS', 'NUMBER_OF_SEATS',\n",
    "       'CARRIER_NAME', 'AIRPORT_FLIGHTS_MONTH', 'AIRLINE_FLIGHTS_MONTH', 'AIRLINE_AIRPORT_FLIGHTS_MONTH', \n",
    "       'AVG_MONTHLY_PASS_AIRPORT', 'AVG_MONTHLY_PASS_AIRLINE', 'FLT_ATTENDANTS_PER_PASS', 'GROUND_SERV_PER_PASS',\n",
    "       'PLANE_AGE', 'DEPARTING_AIRPORT', 'LATITUDE', 'PREVIOUS_AIRPORT', 'PRCP', 'SNOW', 'SNWD', 'AWND']\n",
    "def scale(data, columns):\n",
    "    for col in columns:\n",
    "        data[col] = np.log( data[col] + 1 )\n",
    "scale(df,cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave one - searching the most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "columns = list(df.columns[1:2])+list(df.columns[3:])\n",
    "for column in columns:\n",
    "    \n",
    "#     X = df.drop(['DEP_DEL15', column], axis=1)\n",
    "    X = df[[column, 'MONTH']]\n",
    "    y = df['DEP_DEL15']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    idx = np.where(y_train == 0)[0]\n",
    "    X_train = X_train.drop(X_train.index[idx[:len(idx)//5*4]])\n",
    "    y_train = y_train.drop(y_train.index[idx[:len(idx)//5*4]])\n",
    "\n",
    "    clf = xgb.XGBClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"\\n\\n\\nWithout column: \" + column)\n",
    "    print(\"Train accuracy\", clf.score(X_train, y_train))\n",
    "    print(\"Test accuracy\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried but useless\n",
    "# from sklearn.preprocessing import StandardScaler, normalize\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test  = scaler.fit_transform(X_test)\n",
    "# X_train = normalize(X_train, norm='l2')\n",
    "# X_test  = normalize(X_test, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    277512\n",
       "1     58861\n",
       "Name: DEP_DEL15, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:32:30] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Train accuracy 0.8450707993804497\n",
      "Test accuracy 0.840291651828558\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('DEP_DEL15', axis=1)\n",
    "y = df['DEP_DEL15']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42,shuffle=True)\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "train_acc,t_acc = clf.score(X_train, y_train), clf.score(X_test, y_test)\n",
    "print(\"Train accuracy\",train_acc)\n",
    "print(\"Test accuracy\",t_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.84676467 0.31830075]\n",
      " [0.15323533 0.68169925]]\n"
     ]
    }
   ],
   "source": [
    "y_hat = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "print(cm /np.sum(cm,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr_acc = []\n",
    "test_acc = []\n",
    "X = df.drop('DEP_DEL15', axis=1)\n",
    "y = df['DEP_DEL15']\n",
    "X_tmp, X_test, y_tmp, y_test = train_test_split(X, y, test_size=0.20, random_state=42,shuffle=True)\n",
    "for i in range(1,9):\n",
    "    print(\"curr percentage\",i/10)\n",
    "    X_train,X_val,y_train,y_val = train_test_split(X_tmp, y_tmp, test_size=0.33, random_state=42,shuffle=True)\n",
    "    idx = np.where(y_train == 0)[0]\n",
    "    drop_per = int(len(idx)*i/10)\n",
    "    X_train = X_train.drop(X_train.index[idx[:drop_per]])\n",
    "    y_train = y_train.drop(y_train.index[idx[:drop_per]])\n",
    "\n",
    "    clf = xgb.XGBClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_acc,t_acc = clf.score(X_train, y_train), clf.score(X_val, y_val)\n",
    "    tr_acc.append(train_acc)\n",
    "    test_acc.append(t_acc)\n",
    "    print(\"Train accuracy\",train_acc)\n",
    "    print(\"Test accuracy\",t_acc)\n",
    "    y_hat = clf.predict(X_val)\n",
    "    cm = confusion_matrix(y_val, y_hat)\n",
    "    print(cm /np.sum(cm,axis=0))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr_acc,label=\"train\")\n",
    "plt.plot(test_acc,label=\"test\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Train accuracy\", clf.score(X_train, y_train))\n",
    "print(\"Test accuracy\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "clf = CatBoostClassifier()\n",
    "clf.fit(X_train, y_train, verbose=False)\n",
    "print(\"Train accuracy\", clf.score(X_train, y_train))\n",
    "print(\"Test accuracy\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = clf.predict_proba(X_train)\n",
    "y_test_pred = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "import time\n",
    "import pickle\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LOG_DIR = f\"{int(time.time())}\"\n",
    "\n",
    "# tensorboard = TensorBoard(log_dir=LOG_DIR)\n",
    "\n",
    "def build_model(hp):  # random search passes this hyperparameter() object \n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(Dense(hp.Int('input_units',min_value=50, max_value=200, step=50), \n",
    "                    input_shape=X_train.shape[1:],\n",
    "                    activation='relu'))\n",
    "\n",
    "    model.add( Dropout(hp.Float('dropout_rate',\n",
    "                                    min_value=0.0,\n",
    "                                    max_value=0.5,\n",
    "                                    step=0.1) ) )\n",
    "    \n",
    "    for i in range(hp.Int('n_layers', 1, 2)):  # adding variation of layers.\n",
    "        model.add(Dense(hp.Int(f'layer_{i}_units',\n",
    "                                min_value=10,\n",
    "                                max_value=150,\n",
    "                                step=30), \n",
    "                                activation='relu'))\n",
    "    \n",
    "        model.add( Dropout(hp.Float(f'dropout_{i}_rate',\n",
    "                                    min_value=0.0,\n",
    "                                    max_value=0.5,\n",
    "                                    step=0.1) ) )\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "              optimizer=tensorflow.keras.optimizers.Adam(),\n",
    "              metrics =['accuracy'])\n",
    "              \n",
    "    return model\n",
    "                  \n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='accuracy',\n",
    "    max_trials=10,  # how many variations on model?\n",
    "    executions_per_trial=1) # how many trials per variation? (same model could perform differently)\n",
    "\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "tuner.search(x=X_train,\n",
    "             y=y_train,\n",
    "             epochs=5,\n",
    "             batch_size=64)\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model():  # random search passes this hyperparameter() object \n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(Dense(200, input_shape=X_train.shape[1:], activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add( Dropout( 0.1 ) )\n",
    "    model.add(Dense(70, activation='relu'))\n",
    "    model.add( Dropout( 0.4 ) )\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "              optimizer=tensorflow.keras.optimizers.Adam(),\n",
    "              metrics =['accuracy'])\n",
    "              \n",
    "    return model\n",
    "                  \n",
    "\n",
    "model = build_model()\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, verbose=1)\n",
    "reducer = tf.keras.callbacks.ReduceLROnPlateau( monitor='loss', factor=0.5, patience=3, verbose=1, mode='min')\n",
    "\n",
    "\n",
    "model.fit(X_train,y_train, callbacks=[early_stopping, reducer], batch_size=256,verbose=1,epochs=1000 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score \n",
    "y_hat = model.predict_classes(X_test)\n",
    "# y_test\n",
    "print(\"Test accuracy\", sklearn.metrics.accuracy_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_hat = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "cm /np.sum(cm,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Recall Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "train_precision, train_recall, _ = precision_recall_curve(y_train, y_train_pred[:,1])\n",
    "train_ap = average_precision_score(y_train, y_train_pred[:,1])\n",
    "\n",
    "test_precision, test_recall, _ = precision_recall_curve(y_test, y_test_pred[:,1])\n",
    "test_ap = average_precision_score(y_test, y_test_pred[:,1])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "axs = plt.subplot(2,2,1)\n",
    "axs.set_title('Train Precision Recall Curve')\n",
    "axs.set_xlabel('Recall')\n",
    "axs.set_ylabel('Precission')\n",
    "axs.plot(train_recall, train_precision, label = 'AP = {:.4f}'.format(train_ap))\n",
    "axs.legend()\n",
    "\n",
    "axs = plt.subplot(2,2,2)\n",
    "axs.set_title('Test Precision Recall Curve')\n",
    "axs.set_xlabel('Recall')\n",
    "axs.set_ylabel('Precission')\n",
    "axs.plot(test_recall, test_precision, label = 'AP = {:.4f}'.format(test_ap))\n",
    "axs.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "train_fpr, train_tpr, _ = roc_curve(y_train, y_train_pred[:,1])\n",
    "train_auc = auc(train_fpr, train_tpr)\n",
    "\n",
    "test_fpr, test_tpr, _ = roc_curve(y_test, y_test_pred[:,1])\n",
    "test_auc = auc(test_fpr, test_tpr)\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "axs = plt.subplot(2,2,1)\n",
    "axs.set_title('Train ROC Curve')\n",
    "axs.set_xlabel('False Positive Rate')\n",
    "axs.set_ylabel('True Positive Rate')\n",
    "axs.plot(train_fpr, train_tpr, label = 'AUC = {:.4f}'.format(train_auc))\n",
    "axs.legend()\n",
    "\n",
    "axs = plt.subplot(2,2,2)\n",
    "axs.set_title('Test ROC Curve')\n",
    "axs.set_xlabel('False Positive Rate')\n",
    "axs.set_ylabel('True Positive Rate')\n",
    "axs.plot(test_fpr, test_tpr, label = 'AUC = {:.4f}'.format(test_auc))\n",
    "axs.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
